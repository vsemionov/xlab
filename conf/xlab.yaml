trainer:
  accelerator: auto
  strategy: auto
  devices: auto
  num_nodes: 1
  precision: bf16-mixed
  val_check_interval: 0.2
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm

model:
  max_len: 256
  d_model: 512
  position: xlabml.transformers.PositionalEncoding
  n_layers: 6
  n_heads: 8
  d_ff: 2048
  dropout: 0.1
  prenorm: true
  postnorm: true
  norm: torch.nn.LayerNorm
  activation: torch.nn.GELU
  pos_drop: false
  block_drop: false
  attn_drop: true
  ff_drop: true

data:
  path: wikimedia/wikipedia
  name: 20231101.en
  splits:
    train: 0.9
    val: 0.05
    test: 0.025
    predict: 0  # use remainder
  column: text
  num_tokens: 32768
  tokenizer_path: tokenizers/tokenizer.model
  tokenizer_train_args:
  num_proc: 4
  progress: tqdm
  step_size: 0.5
  batch_size: 256
  pin_memory: true
  num_workers: 4
  persistent_workers: true
