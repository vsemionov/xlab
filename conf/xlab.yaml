trainer:
  accelerator: auto
  strategy: auto
  devices: auto
  num_nodes: 1
  precision: 16-mixed
  max_epochs: 20
model:
  max_len: 128
  d_model: 512
  position: xlab.transformers.PositionalEncoding
  n_layers: 6
  n_heads: 8
  d_ff: 2048
  dropout: 0.1
  prenorm: false
  postnorm: false
  norm: torch.nn.LayerNorm
  activation: torch.nn.ReLU
  attn_drop: true
  ff_drop: true
data:
  path: wikipedia
  name: 20220301.simple
  splits:
    train: 0.9
    val: 0.05
    test: 0.025
    predict: 0  # use remainder
  tokenizer: basic_english
  language: en
  max_tokens: 20_000
  column: text
  num_proc: 4
  progress: tqdm
  chunk_size: 0.5
  batch_size: 256
  pin_memory: true
  num_workers: 4
  persistent_workers: true
optimizer:
  class_path: Adam
  init_args:
    lr: 3e-4
    weight_decay: 0.0
lr_scheduler:
  class_path: ReduceLROnPlateau
  init_args:
    monitor: val_loss
    mode: min
    threshold: 1e-4
    threshold_mode: rel
    factor: 0.1
    patience: 2
    min_lr: 1e-5
